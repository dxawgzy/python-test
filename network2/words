Memcached and Sharding
The design of Memcached illustrates an important principle that is used in several other kinds of
databases, and which you might want to employ in architectures of your own: the clients shard the
database by hashing the keys’ string values and letting the hash determine which member of the cluster
is consulted for each key.
To understand why this is effective, consider a particular key/value pair—like the key sq:42 and the
value 1764 that might be stored by Listing 8–1. To make the best use of the RAM it has available, the
Memcached cluster wants to store this key and value exactly once. But to make the service fast, it wants
to avoid duplication without requiring any coordination between the different servers or
communication between all of the clients.
This means that all of the clients, without any other information to go on than (a) the key and (b) the
list of Memcached servers with which they are configured, need some scheme for working out where
that piece of information belongs. If they fail to make the same decision, then not only might the key and
value be copied on to several servers and reduce the overall memory available, but also a client’s attempt
to remove an invalid entry could leave other invalid copies elsewhere.
The solution is that the clients all implement a single, stable algorithm that can turn a key into an
integer n that selects one of the servers from their list. They do this by using a “hash” algorithm, which
mixes the bits of a string when forming a number so that any pattern in the string is, hopefully,
obliterated.
To see why patterns in key values must be obliterated, consider Listing 8–2. It loads a dictionary of
English words (you might have to download a dictionary of your own or adjust the path to make the
script run on your own machine), and explores how those words would be distributed across four
servers if they were used as keys. The first algorithm tries to divide the alphabet into four roughly equal
